{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebe2f19",
   "metadata": {},
   "source": [
    "# Environment — CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489b8df",
   "metadata": {},
   "source": [
    "The objective is that, by means of the inertia generated by the cart in its horizontal displacements, the pole remains vertical as long as possible. An episode is considered successful when the pole remains vertical for more than 500 timesteps, and an episode is considered unsuccessful if the cart runs off the horizontal guide on the right or left side, or if the pole tilts more than 12 degrees (~0.2095 radians) with respect to the vertical axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee87a1d",
   "metadata": {},
   "source": [
    "It should be noted that, although the allowed value ranges are those shown in the table, an episode will be terminated if:\n",
    "\n",
    "The Cart position on the x axis leaves the (-2.4, 2.4) range\n",
    "The Pole angle leaves the (-0.2095, 0.2095) range\n",
    "Rewards\n",
    "The agent receives a reward of +1 for each time step, with the intention of keeping the pole standing for as long as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "927115d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd680c",
   "metadata": {},
   "source": [
    "The Replay Buffer is implemented as a double-ended queue (deque), and two methods are created to interact with it: save_experience() and sample_experience_batch(). save_experience() allows adding an experience to the replay buffer, while sample_experience_batch() randomly picks up a batch of experiences, which will be used to train the agent. The batch of experiences is returned as a tuple of (states, actions, rewards, next_states, terminals), where each element in the tuple corresponds to an array of {batch_size} items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71799e98",
   "metadata": {},
   "source": [
    "# we use the self at the beginning of each define funtion in order to include it in the object class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b4d94867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "        def __init__(self, state_size, action_size):\n",
    "\n",
    "            self.state_size = state_size\n",
    "            self.action_size = action_size\n",
    "\n",
    "            # Initialize Replay Buffer as python deque\n",
    "            self.replay_buffer = deque(maxlen=100000)\n",
    "\n",
    "            # Set algorithm hyperparameters\n",
    "            self.gamma = 0.99\n",
    "            self.epsilon = 1.\n",
    "            self.epsilon_min = 0.01\n",
    "            self.epsilon_decay = 0.98\n",
    "            self.learning_rate = 0.001\n",
    "            self.update_rate = 10\n",
    "            # Create both Main and Target Networks\n",
    "            self.main_network = self.create_nn()\n",
    "            self.target_network = self.create_nn()\n",
    "            # Initialize Target Network with Main Network's weights\n",
    "            self.update_target_network()\n",
    "            #Creating the neural network. \n",
    "        def create_nn(self):\n",
    "        \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(64, activation='relu', input_dim=self.state_size))\n",
    "            model.add(Dense(64, activation='relu'))\n",
    "            model.add(Dense(self.action_size, activation='linear'))\n",
    "            #this is a continous data so we use 'MSE' as the loss function. \n",
    "            model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "            return model\n",
    "         \n",
    "    \n",
    "        def update_target_network(self):\n",
    "            # Method to set the Main NN's weights on the Target NN\n",
    "            self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "        \n",
    "\n",
    "        def save_experience(self, state, action, reward, next_state, terminal):\n",
    "\n",
    "            # Save the given experience as a (s, a, r, s', terminal) tuple\n",
    "            self.replay_buffer.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        def sample_experience_batch(self, batch_size):\n",
    "\n",
    "            # Sample {batchsize} experiences from the ReplayBuffer\n",
    "            exp_batch = random.sample(self.replay_buffer, batch_size)\n",
    "\n",
    "            # Create an array with the {batchsize} elements for s, a, r, s' and terminal information\n",
    "            state_batch = np.array([batch[0] for batch in exp_batch]).reshape(batch_size, self.state_size)\n",
    "            action_batch = np.array([batch[1] for batch in exp_batch])\n",
    "            reward_batch = [batch[2] for batch in exp_batch]\n",
    "            next_state_batch = np.array([batch[3] for batch in exp_batch]).reshape(batch_size, self.state_size)\n",
    "            terminal_batch = [batch[4] for batch in exp_batch]\n",
    "\n",
    "            # Return a tuple, where each item corresponds to each array/batch created above\n",
    "            return state_batch, action_batch, reward_batch, next_state_batch, terminal_batch\n",
    "\n",
    "        def pick_epsilon_greedy_action(self, state):\n",
    "\n",
    "            # Pick random action with probability ε\n",
    "            if random.uniform(0, 1) < self.epsilon:\n",
    "                return np.random.randint(self.action_size)\n",
    "\n",
    "            # Pick action with highest Q-Value (item with highest value for Main NN's output)\n",
    "            state = state.reshape((1, self.state_size))\n",
    "            q_values = self.main_network.predict(state, verbose=0)\n",
    "            return np.argmax(q_values[0])\n",
    "\n",
    "        def train(self, batch_size):\n",
    "\n",
    "            # Sample a batch of experiences\n",
    "            state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = self.sample_experience_batch(batch_size)\n",
    "\n",
    "            # Get the actions with highest Q-Value for the batch of next states\n",
    "            next_q = self.target_network.predict(next_state_batch, verbose=0)\n",
    "            #Return the maximum of an array\n",
    "            max_next_q = np.amax(next_q, axis=1)\n",
    "            # Get the Q-values of each state in the batch of states\n",
    "            q_values = self.main_network.predict(state_batch, verbose=0)\n",
    "\n",
    "            # Update the Q-Value corresponding to the current action with the Target Value\n",
    "            for i in range(batch_size):\n",
    "                q_values[i][action_batch[i]] = reward_batch[i] if terminal_batch[i] else reward_batch[i] + self.gamma * max_next_q[i]\n",
    "\n",
    "            # Fit the network\n",
    "            self.main_network.fit(state_batch, q_values, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e763da",
   "metadata": {},
   "source": [
    "Both the Main and Target Neural Networks are initialized with the create_nn() method, which creates a keras model with 3 layers of 64, 64 and 2 (action size for CartPole environment) neurons each, and whose input is the state size of the environment. The loss function used is the Mean Squared Error (MSE) and the optimizer is Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c3265",
   "metadata": {},
   "source": [
    "The action with the best Q-Value, which is the one with highest value on the output of the main neural network, is chosen with probability 1-ε, and a random action is chosen otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7fd30",
   "metadata": {},
   "source": [
    "The full algorithm implementation, updates the epsilon value in each episode, making it lower and lower. This makes the exploration phase happen mainly at the beginning of the training, and the exploitation phase in the remaining episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d28fe6",
   "metadata": {},
   "source": [
    "# Perform action on environment and store experience in Replay Buffer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d77aaf",
   "metadata": {},
   "source": [
    "The action extracted from the previous step is applied on the gym environment via the env.step() method, which receives the action to be applied as parameter. This method returns a tuple (next_state, reward, terminated, truncated, info), from which the next state, reward and terminated fields are used together with the current state and the action chosen for saving the experience in the Replay Buffer with the save_experience() method defined before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707005d9",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e28b99",
   "metadata": {},
   "source": [
    "Finally, the agent is trained with the experiences stored along the episodes.The output of the main neural network is taken as predicted value, and the target value is calculated from the reward and the output of the target network for the action with highest Q-Value on the next state. The loss is then calculated as the squared difference between the predicted value and the target value, and gradient descent is performed on the main neural network from this loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57738d7f",
   "metadata": {},
   "source": [
    "# 1. Initialize Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1746b35",
   "metadata": {},
   "source": [
    "After having defined the behavior of the algorithm in each of the previous steps, as well as its implementation in code, all the pieces can be put together and the DQN algorithm can be built, as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "26d7eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define state and action size\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f8b322a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fa5eecf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d7befc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CartPole environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a68edb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size_total = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8647fbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is not discrete, it is a continuos state. \n",
    "state_size_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e88df340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d48c0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define number of episodes, timesteps per episode and batch size\n",
    "num_episodes = 150\n",
    "num_timesteps = 500\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4339871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initlaize DQNAgent\n",
    "dqn_agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb79bf",
   "metadata": {},
   "source": [
    "# Implement and execute flow of DQN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399fe07",
   "metadata": {},
   "source": [
    "dqn_agent.function_name() is when you use a function from the Class object. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abfa18",
   "metadata": {},
   "source": [
    "# Below is when we start to train the agent into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2f4b8f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on EPISODE 1 with epsilon 1.0\n",
      "Episode:  1  terminated with Reward  16.0\n",
      "Time elapsed during EPISODE 1: 0.02395009994506836 seconds = 0.0 minutes\n",
      "\n",
      "Training on EPISODE 2 with epsilon 0.98\n",
      "Episode:  2  terminated with Reward  18.0\n",
      "Time elapsed during EPISODE 2: 0.014440774917602539 seconds = 0.0 minutes\n",
      "\n",
      "Training on EPISODE 3 with epsilon 0.9603999999999999\n",
      "Episode:  3  terminated with Reward  12.0\n",
      "Time elapsed during EPISODE 3: 0.002942800521850586 seconds = 0.0 minutes\n",
      "\n",
      "Training on EPISODE 4 with epsilon 0.9411919999999999\n",
      "Episode:  4  terminated with Reward  19.0\n",
      "Time elapsed during EPISODE 4: 0.005011320114135742 seconds = 0.0 minutes\n",
      "\n",
      "Training on EPISODE 5 with epsilon 0.9223681599999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anacondalastone\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  5  terminated with Reward  30.0\n",
      "Time elapsed during EPISODE 5: 0.946674108505249 seconds = 0.016 minutes\n",
      "\n",
      "Training on EPISODE 6 with epsilon 0.9039207967999998\n",
      "Episode:  6  terminated with Reward  25.0\n",
      "Time elapsed during EPISODE 6: 0.18288612365722656 seconds = 0.003 minutes\n",
      "\n",
      "Training on EPISODE 7 with epsilon 0.8858423808639998\n",
      "Episode:  7  terminated with Reward  15.0\n",
      "Time elapsed during EPISODE 7: 0.1250760555267334 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 8 with epsilon 0.8681255332467198\n",
      "Episode:  8  terminated with Reward  18.0\n",
      "Time elapsed during EPISODE 8: 0.15013957023620605 seconds = 0.003 minutes\n",
      "\n",
      "Training on EPISODE 9 with epsilon 0.8507630225817854\n",
      "Episode:  9  terminated with Reward  15.0\n",
      "Time elapsed during EPISODE 9: 0.12001276016235352 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 10 with epsilon 0.8337477621301497\n",
      "Episode:  10  terminated with Reward  15.0\n",
      "Time elapsed during EPISODE 10: 0.10402131080627441 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 11 with epsilon 0.8170728068875467\n",
      "Episode:  11  terminated with Reward  24.0\n",
      "Time elapsed during EPISODE 11: 0.1544480323791504 seconds = 0.003 minutes\n",
      "\n",
      "Training on EPISODE 12 with epsilon 0.8007313507497957\n",
      "Episode:  12  terminated with Reward  9.0\n",
      "Time elapsed during EPISODE 12: 0.06901717185974121 seconds = 0.001 minutes\n",
      "\n",
      "Training on EPISODE 13 with epsilon 0.7847167237347998\n",
      "Episode:  13  terminated with Reward  10.0\n",
      "Time elapsed during EPISODE 13: 0.07620453834533691 seconds = 0.001 minutes\n",
      "\n",
      "Training on EPISODE 14 with epsilon 0.7690223892601038\n",
      "Episode:  14  terminated with Reward  12.0\n",
      "Time elapsed during EPISODE 14: 0.10648989677429199 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 15 with epsilon 0.7536419414749017\n",
      "Episode:  15  terminated with Reward  12.0\n",
      "Time elapsed during EPISODE 15: 0.1181631088256836 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 16 with epsilon 0.7385691026454037\n",
      "Episode:  16  terminated with Reward  26.0\n",
      "Time elapsed during EPISODE 16: 0.23366641998291016 seconds = 0.004 minutes\n",
      "\n",
      "Training on EPISODE 17 with epsilon 0.7237977205924956\n",
      "Episode:  17  terminated with Reward  15.0\n",
      "Time elapsed during EPISODE 17: 0.13612103462219238 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 18 with epsilon 0.7093217661806457\n",
      "Episode:  18  terminated with Reward  17.0\n",
      "Time elapsed during EPISODE 18: 0.13613486289978027 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 19 with epsilon 0.6951353308570327\n",
      "Episode:  19  terminated with Reward  23.0\n",
      "Time elapsed during EPISODE 19: 0.15779781341552734 seconds = 0.003 minutes\n",
      "\n",
      "Training on EPISODE 20 with epsilon 0.6812326242398921\n",
      "Episode:  20  terminated with Reward  11.0\n",
      "Time elapsed during EPISODE 20: 0.09192180633544922 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 21 with epsilon 0.6676079717550942\n",
      "Episode:  21  terminated with Reward  14.0\n",
      "Time elapsed during EPISODE 21: 0.08842778205871582 seconds = 0.001 minutes\n",
      "\n",
      "Training on EPISODE 22 with epsilon 0.6542558123199923\n",
      "Episode:  22  terminated with Reward  10.0\n",
      "Time elapsed during EPISODE 22: 0.06282258033752441 seconds = 0.001 minutes\n",
      "\n",
      "Training on EPISODE 23 with epsilon 0.6411706960735924\n",
      "Episode:  23  terminated with Reward  16.0\n",
      "Time elapsed during EPISODE 23: 0.12236380577087402 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 24 with epsilon 0.6283472821521205\n",
      "Episode:  24  terminated with Reward  16.0\n",
      "Time elapsed during EPISODE 24: 0.10982680320739746 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 25 with epsilon 0.6157803365090782\n",
      "Episode:  25  terminated with Reward  13.0\n",
      "Time elapsed during EPISODE 25: 0.07856512069702148 seconds = 0.001 minutes\n",
      "\n",
      "Training on EPISODE 26 with epsilon 0.6034647297788965\n",
      "Episode:  26  terminated with Reward  20.0\n",
      "Time elapsed during EPISODE 26: 0.14609837532043457 seconds = 0.002 minutes\n",
      "\n",
      "Training on EPISODE 27 with epsilon 0.5913954351833186\n",
      "Episode:  27  terminated with Reward  29.0\n",
      "Time elapsed during EPISODE 27: 0.2122478485107422 seconds = 0.004 minutes\n",
      "\n",
      "Training on EPISODE 28 with epsilon 0.5795675264796523\n",
      "Episode:  28  terminated with Reward  27.0\n",
      "Time elapsed during EPISODE 28: 0.1771085262298584 seconds = 0.003 minutes\n",
      "\n",
      "Training on EPISODE 29 with epsilon 0.5679761759500592\n",
      "Episode:  29  terminated with Reward  22.0\n",
      "Time elapsed during EPISODE 29: 0.15478825569152832 seconds = 0.003 minutes\n",
      "\n",
      "Training on EPISODE 30 with epsilon 0.5566166524310581\n",
      "Episode:  30  terminated with Reward  53.0\n",
      "Time elapsed during EPISODE 30: 0.40613746643066406 seconds = 0.007 minutes\n",
      "\n",
      "Training on EPISODE 31 with epsilon 0.5454843193824369\n",
      "Episode:  31  terminated with Reward  47.0\n",
      "Time elapsed during EPISODE 31: 0.3506896495819092 seconds = 0.006 minutes\n",
      "\n",
      "Training on EPISODE 32 with epsilon 0.5345746329947881\n",
      "Episode:  32  terminated with Reward  11.0\n",
      "Time elapsed during EPISODE 32: 0.0823967456817627 seconds = 0.001 minutes\n",
      "\n",
      "Training on EPISODE 33 with epsilon 0.5238831403348924\n",
      "Episode:  33  terminated with Reward  77.0\n",
      "Time elapsed during EPISODE 33: 0.5775082111358643 seconds = 0.01 minutes\n",
      "\n",
      "Training on EPISODE 34 with epsilon 0.5134054775281945\n",
      "Episode:  34  terminated with Reward  41.0\n",
      "Time elapsed during EPISODE 34: 0.2997159957885742 seconds = 0.005 minutes\n",
      "\n",
      "Training on EPISODE 35 with epsilon 0.5031373679776306\n",
      "Episode:  35  terminated with Reward  97.0\n",
      "Time elapsed during EPISODE 35: 0.73748779296875 seconds = 0.012 minutes\n",
      "\n",
      "Training on EPISODE 36 with epsilon 0.493074620618078\n",
      "Episode:  36  terminated with Reward  10.0\n",
      "Time elapsed during EPISODE 36: 0.08295893669128418 seconds = 0.001 minutes\n",
      "\n",
      "Training on EPISODE 37 with epsilon 0.48321312820571644\n",
      "Episode:  37  terminated with Reward  133.0\n",
      "Time elapsed during EPISODE 37: 1.0614652633666992 seconds = 0.018 minutes\n",
      "\n",
      "Training on EPISODE 38 with epsilon 0.4735488656416021\n",
      "Episode:  38  terminated with Reward  102.0\n",
      "Time elapsed during EPISODE 38: 0.8201897144317627 seconds = 0.014 minutes\n",
      "\n",
      "Training on EPISODE 39 with epsilon 0.46407788832877006\n",
      "Episode:  39  terminated with Reward  53.0\n",
      "Time elapsed during EPISODE 39: 0.405895471572876 seconds = 0.007 minutes\n",
      "\n",
      "Training on EPISODE 40 with epsilon 0.45479633056219465\n",
      "Episode:  40  terminated with Reward  83.0\n",
      "Time elapsed during EPISODE 40: 0.6204433441162109 seconds = 0.01 minutes\n",
      "\n",
      "Training on EPISODE 41 with epsilon 0.44570040395095073\n",
      "Episode:  41  terminated with Reward  183.0\n",
      "Time elapsed during EPISODE 41: 1.4247446060180664 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 42 with epsilon 0.4367863958719317\n",
      "Episode:  42  terminated with Reward  97.0\n",
      "Time elapsed during EPISODE 42: 0.7924606800079346 seconds = 0.013 minutes\n",
      "\n",
      "Training on EPISODE 43 with epsilon 0.42805066795449304\n",
      "Episode:  43  terminated with Reward  127.0\n",
      "Time elapsed during EPISODE 43: 0.9815640449523926 seconds = 0.016 minutes\n",
      "\n",
      "Training on EPISODE 44 with epsilon 0.41948965459540316\n",
      "Episode:  44  terminated with Reward  275.0\n",
      "Time elapsed during EPISODE 44: 2.0875120162963867 seconds = 0.035 minutes\n",
      "\n",
      "Training on EPISODE 45 with epsilon 0.4110998615034951\n",
      "Episode:  45  terminated with Reward  339.0\n",
      "Time elapsed during EPISODE 45: 2.9004132747650146 seconds = 0.048 minutes\n",
      "\n",
      "Training on EPISODE 46 with epsilon 0.4028778642734252\n",
      "Episode:  46  terminated with Reward  264.0\n",
      "Time elapsed during EPISODE 46: 2.3427352905273438 seconds = 0.039 minutes\n",
      "\n",
      "Training on EPISODE 47 with epsilon 0.39482030698795667\n",
      "Episode:  47  terminated with Reward  232.0\n",
      "Time elapsed during EPISODE 47: 2.029280662536621 seconds = 0.034 minutes\n",
      "\n",
      "Training on EPISODE 48 with epsilon 0.38692390084819756\n",
      "Episode:  48  terminated with Reward  267.0\n",
      "Time elapsed during EPISODE 48: 2.5344128608703613 seconds = 0.042 minutes\n",
      "\n",
      "Training on EPISODE 49 with epsilon 0.3791854228312336\n",
      "Episode:  49  terminated with Reward  281.0\n",
      "Time elapsed during EPISODE 49: 2.7472217082977295 seconds = 0.046 minutes\n",
      "\n",
      "Training on EPISODE 50 with epsilon 0.37160171437460887\n",
      "Episode:  50  terminated with Reward  227.0\n",
      "Time elapsed during EPISODE 50: 2.365044593811035 seconds = 0.039 minutes\n",
      "\n",
      "Training on EPISODE 51 with epsilon 0.3641696800871167\n",
      "Episode:  51  terminated with Reward  232.0\n",
      "Time elapsed during EPISODE 51: 1.9310386180877686 seconds = 0.032 minutes\n",
      "\n",
      "Training on EPISODE 52 with epsilon 0.35688628648537435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  52  terminated with Reward  245.0\n",
      "Time elapsed during EPISODE 52: 1.9839544296264648 seconds = 0.033 minutes\n",
      "\n",
      "Training on EPISODE 53 with epsilon 0.34974856075566685\n",
      "Episode:  53  terminated with Reward  268.0\n",
      "Time elapsed during EPISODE 53: 2.3124778270721436 seconds = 0.039 minutes\n",
      "\n",
      "Training on EPISODE 54 with epsilon 0.3427535895405535\n",
      "Episode:  54  terminated with Reward  210.0\n",
      "Time elapsed during EPISODE 54: 1.7425265312194824 seconds = 0.029 minutes\n",
      "\n",
      "Training on EPISODE 55 with epsilon 0.33589851774974244\n",
      "Episode:  55  terminated with Reward  269.0\n",
      "Time elapsed during EPISODE 55: 2.3365819454193115 seconds = 0.039 minutes\n",
      "\n",
      "Training on EPISODE 56 with epsilon 0.3291805473947476\n",
      "Episode:  56  terminated with Reward  205.0\n",
      "Time elapsed during EPISODE 56: 1.7598390579223633 seconds = 0.029 minutes\n",
      "\n",
      "Training on EPISODE 57 with epsilon 0.32259693644685267\n",
      "Episode:  57  terminated with Reward  349.0\n",
      "Time elapsed during EPISODE 57: 3.0700490474700928 seconds = 0.051 minutes\n",
      "\n",
      "Training on EPISODE 58 with epsilon 0.3161449977179156\n",
      "Episode:  58  terminated with Reward  408.0\n",
      "Time elapsed during EPISODE 58: 3.7290711402893066 seconds = 0.062 minutes\n",
      "\n",
      "Training on EPISODE 59 with epsilon 0.3098220977635573\n",
      "Episode:  59  terminated with Reward  247.0\n",
      "Time elapsed during EPISODE 59: 2.263822317123413 seconds = 0.038 minutes\n",
      "\n",
      "Training on EPISODE 60 with epsilon 0.30362565580828615\n",
      "Episode:  60  terminated with Reward  196.0\n",
      "Time elapsed during EPISODE 60: 1.61696457862854 seconds = 0.027 minutes\n",
      "\n",
      "Training on EPISODE 61 with epsilon 0.2975531426921204\n",
      "Episode:  61  terminated with Reward  306.0\n",
      "Time elapsed during EPISODE 61: 2.7508344650268555 seconds = 0.046 minutes\n",
      "\n",
      "Training on EPISODE 62 with epsilon 0.291602079838278\n",
      "Episode:  62  terminated with Reward  280.0\n",
      "Time elapsed during EPISODE 62: 2.530911684036255 seconds = 0.042 minutes\n",
      "\n",
      "Training on EPISODE 63 with epsilon 0.2857700382415124\n",
      "Episode:  63  terminated with Reward  283.0\n",
      "Time elapsed during EPISODE 63: 2.5119283199310303 seconds = 0.042 minutes\n",
      "\n",
      "Training on EPISODE 64 with epsilon 0.2800546374766822\n",
      "Episode:  64  terminated with Reward  251.0\n",
      "Time elapsed during EPISODE 64: 2.066484212875366 seconds = 0.034 minutes\n",
      "\n",
      "Training on EPISODE 65 with epsilon 0.27445354472714856\n",
      "Episode:  65  terminated with Reward  362.0\n",
      "Time elapsed during EPISODE 65: 3.07183837890625 seconds = 0.051 minutes\n",
      "\n",
      "Training on EPISODE 66 with epsilon 0.2689644738326056\n",
      "Episode:  66  terminated with Reward  209.0\n",
      "Time elapsed during EPISODE 66: 1.7774367332458496 seconds = 0.03 minutes\n",
      "\n",
      "Training on EPISODE 67 with epsilon 0.26358518435595346\n",
      "Episode:  67  terminated with Reward  155.0\n",
      "Time elapsed during EPISODE 67: 1.4038779735565186 seconds = 0.023 minutes\n",
      "\n",
      "Training on EPISODE 68 with epsilon 0.25831348066883436\n",
      "Episode:  68  terminated with Reward  229.0\n",
      "Time elapsed during EPISODE 68: 1.9646601676940918 seconds = 0.033 minutes\n",
      "\n",
      "Training on EPISODE 69 with epsilon 0.2531472110554577\n",
      "Episode:  69  terminated with Reward  170.0\n",
      "Time elapsed during EPISODE 69: 1.4568841457366943 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 70 with epsilon 0.24808426683434853\n",
      "Episode:  70  terminated with Reward  151.0\n",
      "Time elapsed during EPISODE 70: 1.4499866962432861 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 71 with epsilon 0.24312258149766156\n",
      "Episode:  71  terminated with Reward  152.0\n",
      "Time elapsed during EPISODE 71: 1.3364887237548828 seconds = 0.022 minutes\n",
      "\n",
      "Training on EPISODE 72 with epsilon 0.2382601298677083\n",
      "Episode:  72  terminated with Reward  223.0\n",
      "Time elapsed during EPISODE 72: 1.9724175930023193 seconds = 0.033 minutes\n",
      "\n",
      "Training on EPISODE 73 with epsilon 0.23349492727035415\n",
      "Episode:  73  terminated with Reward  335.0\n",
      "Time elapsed during EPISODE 73: 2.897209405899048 seconds = 0.048 minutes\n",
      "\n",
      "Training on EPISODE 74 with epsilon 0.22882502872494706\n",
      "Episode:  74  terminated with Reward  238.0\n",
      "Time elapsed during EPISODE 74: 2.0430428981781006 seconds = 0.034 minutes\n",
      "\n",
      "Training on EPISODE 75 with epsilon 0.22424852815044813\n",
      "Episode:  75  terminated with Reward  168.0\n",
      "Time elapsed during EPISODE 75: 1.4512996673583984 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 76 with epsilon 0.21976355758743915\n",
      "Episode:  76  terminated with Reward  364.0\n",
      "Time elapsed during EPISODE 76: 3.2581825256347656 seconds = 0.054 minutes\n",
      "\n",
      "Training on EPISODE 77 with epsilon 0.21536828643569036\n",
      "Episode:  77  terminated with Reward  261.0\n",
      "Time elapsed during EPISODE 77: 2.201890230178833 seconds = 0.037 minutes\n",
      "\n",
      "Training on EPISODE 78 with epsilon 0.21106092070697655\n",
      "Episode:  78  terminated with Reward  262.0\n",
      "Time elapsed during EPISODE 78: 2.441556453704834 seconds = 0.041 minutes\n",
      "\n",
      "Training on EPISODE 79 with epsilon 0.20683970229283702\n",
      "Episode:  79  terminated with Reward  220.0\n",
      "Time elapsed during EPISODE 79: 1.888941764831543 seconds = 0.031 minutes\n",
      "\n",
      "Training on EPISODE 80 with epsilon 0.20270290824698028\n",
      "Episode:  80  terminated with Reward  301.0\n",
      "Time elapsed during EPISODE 80: 2.5661933422088623 seconds = 0.043 minutes\n",
      "\n",
      "Training on EPISODE 81 with epsilon 0.19864885008204067\n",
      "Episode:  81  terminated with Reward  309.0\n",
      "Time elapsed during EPISODE 81: 2.6342718601226807 seconds = 0.044 minutes\n",
      "\n",
      "Training on EPISODE 82 with epsilon 0.19467587308039985\n",
      "Episode:  82  terminated with Reward  273.0\n",
      "Time elapsed during EPISODE 82: 2.394420623779297 seconds = 0.04 minutes\n",
      "\n",
      "Training on EPISODE 83 with epsilon 0.19078235561879187\n",
      "Episode:  83  terminated with Reward  191.0\n",
      "Time elapsed during EPISODE 83: 1.5696580410003662 seconds = 0.026 minutes\n",
      "\n",
      "Training on EPISODE 84 with epsilon 0.18696670850641603\n",
      "Episode:  84  terminated with Reward  210.0\n",
      "Time elapsed during EPISODE 84: 1.7968759536743164 seconds = 0.03 minutes\n",
      "\n",
      "Training on EPISODE 85 with epsilon 0.18322737433628772\n",
      "Episode:  85  terminated with Reward  227.0\n",
      "Time elapsed during EPISODE 85: 1.9335851669311523 seconds = 0.032 minutes\n",
      "\n",
      "Training on EPISODE 86 with epsilon 0.17956282684956196\n",
      "Episode:  86  terminated with Reward  248.0\n",
      "Time elapsed during EPISODE 86: 2.1045784950256348 seconds = 0.035 minutes\n",
      "\n",
      "Training on EPISODE 87 with epsilon 0.1759715703125707\n",
      "Episode:  87  terminated with Reward  241.0\n",
      "Time elapsed during EPISODE 87: 2.166933536529541 seconds = 0.036 minutes\n",
      "\n",
      "Training on EPISODE 88 with epsilon 0.1724521389063193\n",
      "Time elapsed during EPISODE 88: 4.811986684799194 seconds = 0.08 minutes\n",
      "\n",
      "Training on EPISODE 89 with epsilon 0.16900309612819292\n",
      "Time elapsed during EPISODE 89: 4.308694362640381 seconds = 0.072 minutes\n",
      "\n",
      "Training on EPISODE 90 with epsilon 0.16562303420562907\n",
      "Episode:  90  terminated with Reward  498.0\n",
      "Time elapsed during EPISODE 90: 4.722156524658203 seconds = 0.079 minutes\n",
      "\n",
      "Training on EPISODE 91 with epsilon 0.16231057352151648\n",
      "Episode:  91  terminated with Reward  305.0\n",
      "Time elapsed during EPISODE 91: 2.662700653076172 seconds = 0.044 minutes\n",
      "\n",
      "Training on EPISODE 92 with epsilon 0.15906436205108615\n",
      "Episode:  92  terminated with Reward  202.0\n",
      "Time elapsed during EPISODE 92: 1.807647466659546 seconds = 0.03 minutes\n",
      "\n",
      "Training on EPISODE 93 with epsilon 0.15588307481006441\n",
      "Episode:  93  terminated with Reward  208.0\n",
      "Time elapsed during EPISODE 93: 1.9884800910949707 seconds = 0.033 minutes\n",
      "\n",
      "Training on EPISODE 94 with epsilon 0.15276541331386312\n",
      "Episode:  94  terminated with Reward  215.0\n",
      "Time elapsed during EPISODE 94: 1.8402562141418457 seconds = 0.031 minutes\n",
      "\n",
      "Training on EPISODE 95 with epsilon 0.14971010504758586\n",
      "Episode:  95  terminated with Reward  208.0\n",
      "Time elapsed during EPISODE 95: 1.839832067489624 seconds = 0.031 minutes\n",
      "\n",
      "Training on EPISODE 96 with epsilon 0.14671590294663414\n",
      "Episode:  96  terminated with Reward  258.0\n",
      "Time elapsed during EPISODE 96: 2.2178852558135986 seconds = 0.037 minutes\n",
      "\n",
      "Training on EPISODE 97 with epsilon 0.14378158488770146\n",
      "Episode:  97  terminated with Reward  242.0\n",
      "Time elapsed during EPISODE 97: 2.0768487453460693 seconds = 0.035 minutes\n",
      "\n",
      "Training on EPISODE 98 with epsilon 0.14090595318994742\n",
      "Episode:  98  terminated with Reward  197.0\n",
      "Time elapsed during EPISODE 98: 1.7193303108215332 seconds = 0.029 minutes\n",
      "\n",
      "Training on EPISODE 99 with epsilon 0.13808783412614847\n",
      "Episode:  99  terminated with Reward  197.0\n",
      "Time elapsed during EPISODE 99: 1.73270583152771 seconds = 0.029 minutes\n",
      "\n",
      "Training on EPISODE 100 with epsilon 0.13532607744362551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  100  terminated with Reward  200.0\n",
      "Time elapsed during EPISODE 100: 1.6809828281402588 seconds = 0.028 minutes\n",
      "\n",
      "Training on EPISODE 101 with epsilon 0.132619555894753\n",
      "Episode:  101  terminated with Reward  173.0\n",
      "Time elapsed during EPISODE 101: 1.4665937423706055 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 102 with epsilon 0.12996716477685794\n",
      "Episode:  102  terminated with Reward  172.0\n",
      "Time elapsed during EPISODE 102: 1.591362476348877 seconds = 0.027 minutes\n",
      "\n",
      "Training on EPISODE 103 with epsilon 0.12736782148132078\n",
      "Episode:  103  terminated with Reward  177.0\n",
      "Time elapsed during EPISODE 103: 1.4972434043884277 seconds = 0.025 minutes\n",
      "\n",
      "Training on EPISODE 104 with epsilon 0.12482046505169436\n",
      "Episode:  104  terminated with Reward  181.0\n",
      "Time elapsed during EPISODE 104: 1.5261626243591309 seconds = 0.025 minutes\n",
      "\n",
      "Training on EPISODE 105 with epsilon 0.12232405575066048\n",
      "Episode:  105  terminated with Reward  178.0\n",
      "Time elapsed during EPISODE 105: 1.587895154953003 seconds = 0.026 minutes\n",
      "\n",
      "Training on EPISODE 106 with epsilon 0.11987757463564727\n",
      "Episode:  106  terminated with Reward  182.0\n",
      "Time elapsed during EPISODE 106: 1.6021173000335693 seconds = 0.027 minutes\n",
      "\n",
      "Training on EPISODE 107 with epsilon 0.11748002314293432\n",
      "Episode:  107  terminated with Reward  166.0\n",
      "Time elapsed during EPISODE 107: 1.463052749633789 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 108 with epsilon 0.11513042268007563\n",
      "Episode:  108  terminated with Reward  167.0\n",
      "Time elapsed during EPISODE 108: 1.461397647857666 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 109 with epsilon 0.11282781422647412\n",
      "Episode:  109  terminated with Reward  192.0\n",
      "Time elapsed during EPISODE 109: 1.7030820846557617 seconds = 0.028 minutes\n",
      "\n",
      "Training on EPISODE 110 with epsilon 0.11057125794194463\n",
      "Episode:  110  terminated with Reward  219.0\n",
      "Time elapsed during EPISODE 110: 2.0525169372558594 seconds = 0.034 minutes\n",
      "\n",
      "Training on EPISODE 111 with epsilon 0.10835983278310574\n",
      "Episode:  111  terminated with Reward  201.0\n",
      "Time elapsed during EPISODE 111: 1.736290454864502 seconds = 0.029 minutes\n",
      "\n",
      "Training on EPISODE 112 with epsilon 0.10619263612744362\n",
      "Episode:  112  terminated with Reward  241.0\n",
      "Time elapsed during EPISODE 112: 2.1121065616607666 seconds = 0.035 minutes\n",
      "\n",
      "Training on EPISODE 113 with epsilon 0.10406878340489474\n",
      "Episode:  113  terminated with Reward  240.0\n",
      "Time elapsed during EPISODE 113: 2.142409324645996 seconds = 0.036 minutes\n",
      "\n",
      "Training on EPISODE 114 with epsilon 0.10198740773679685\n",
      "Episode:  114  terminated with Reward  208.0\n",
      "Time elapsed during EPISODE 114: 1.9056150913238525 seconds = 0.032 minutes\n",
      "\n",
      "Training on EPISODE 115 with epsilon 0.09994765958206091\n",
      "Episode:  115  terminated with Reward  252.0\n",
      "Time elapsed during EPISODE 115: 2.2042641639709473 seconds = 0.037 minutes\n",
      "\n",
      "Training on EPISODE 116 with epsilon 0.0979487063904197\n",
      "Episode:  116  terminated with Reward  183.0\n",
      "Time elapsed during EPISODE 116: 1.5257134437561035 seconds = 0.025 minutes\n",
      "\n",
      "Training on EPISODE 117 with epsilon 0.0959897322626113\n",
      "Episode:  117  terminated with Reward  228.0\n",
      "Time elapsed during EPISODE 117: 1.9654743671417236 seconds = 0.033 minutes\n",
      "\n",
      "Training on EPISODE 118 with epsilon 0.09406993761735907\n",
      "Episode:  118  terminated with Reward  219.0\n",
      "Time elapsed during EPISODE 118: 1.871664047241211 seconds = 0.031 minutes\n",
      "\n",
      "Training on EPISODE 119 with epsilon 0.0921885388650119\n",
      "Episode:  119  terminated with Reward  351.0\n",
      "Time elapsed during EPISODE 119: 3.47285795211792 seconds = 0.058 minutes\n",
      "\n",
      "Training on EPISODE 120 with epsilon 0.09034476808771166\n",
      "Episode:  120  terminated with Reward  199.0\n",
      "Time elapsed during EPISODE 120: 1.8432915210723877 seconds = 0.031 minutes\n",
      "\n",
      "Training on EPISODE 121 with epsilon 0.08853787272595742\n",
      "Episode:  121  terminated with Reward  261.0\n",
      "Time elapsed during EPISODE 121: 2.2101798057556152 seconds = 0.037 minutes\n",
      "\n",
      "Training on EPISODE 122 with epsilon 0.08676711527143827\n",
      "Episode:  122  terminated with Reward  296.0\n",
      "Time elapsed during EPISODE 122: 2.720860242843628 seconds = 0.045 minutes\n",
      "\n",
      "Training on EPISODE 123 with epsilon 0.0850317729660095\n",
      "Episode:  123  terminated with Reward  249.0\n",
      "Time elapsed during EPISODE 123: 2.470210552215576 seconds = 0.041 minutes\n",
      "\n",
      "Training on EPISODE 124 with epsilon 0.08333113750668932\n",
      "Episode:  124  terminated with Reward  252.0\n",
      "Time elapsed during EPISODE 124: 2.4091110229492188 seconds = 0.04 minutes\n",
      "\n",
      "Training on EPISODE 125 with epsilon 0.08166451475655553\n",
      "Episode:  125  terminated with Reward  284.0\n",
      "Time elapsed during EPISODE 125: 2.546215534210205 seconds = 0.042 minutes\n",
      "\n",
      "Training on EPISODE 126 with epsilon 0.08003122446142442\n",
      "Episode:  126  terminated with Reward  332.0\n",
      "Time elapsed during EPISODE 126: 3.0371596813201904 seconds = 0.051 minutes\n",
      "\n",
      "Training on EPISODE 127 with epsilon 0.07843059997219594\n",
      "Time elapsed during EPISODE 127: 4.508876085281372 seconds = 0.075 minutes\n",
      "\n",
      "Training on EPISODE 128 with epsilon 0.07686198797275202\n",
      "Time elapsed during EPISODE 128: 5.3509626388549805 seconds = 0.089 minutes\n",
      "\n",
      "Training on EPISODE 129 with epsilon 0.07532474821329699\n",
      "Time elapsed during EPISODE 129: 5.021876811981201 seconds = 0.084 minutes\n",
      "\n",
      "Training on EPISODE 130 with epsilon 0.07381825324903105\n",
      "Time elapsed during EPISODE 130: 5.056938171386719 seconds = 0.084 minutes\n",
      "\n",
      "Training on EPISODE 131 with epsilon 0.07234188818405042\n",
      "Time elapsed during EPISODE 131: 4.869222640991211 seconds = 0.081 minutes\n",
      "\n",
      "Training on EPISODE 132 with epsilon 0.07089505042036941\n",
      "Time elapsed during EPISODE 132: 4.54512619972229 seconds = 0.076 minutes\n",
      "\n",
      "Training on EPISODE 133 with epsilon 0.06947714941196202\n",
      "Episode:  133  terminated with Reward  488.0\n",
      "Time elapsed during EPISODE 133: 4.373083591461182 seconds = 0.073 minutes\n",
      "\n",
      "Training on EPISODE 134 with epsilon 0.06808760642372277\n",
      "Episode:  134  terminated with Reward  125.0\n",
      "Time elapsed during EPISODE 134: 1.0858116149902344 seconds = 0.018 minutes\n",
      "\n",
      "Training on EPISODE 135 with epsilon 0.06672585429524831\n",
      "Episode:  135  terminated with Reward  337.0\n",
      "Time elapsed during EPISODE 135: 3.300058126449585 seconds = 0.055 minutes\n",
      "\n",
      "Training on EPISODE 136 with epsilon 0.06539133720934334\n",
      "Episode:  136  terminated with Reward  148.0\n",
      "Time elapsed during EPISODE 136: 1.4638147354125977 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 137 with epsilon 0.06408351046515648\n",
      "Episode:  137  terminated with Reward  154.0\n",
      "Time elapsed during EPISODE 137: 1.374553918838501 seconds = 0.023 minutes\n",
      "\n",
      "Training on EPISODE 138 with epsilon 0.06280184025585335\n",
      "Episode:  138  terminated with Reward  128.0\n",
      "Time elapsed during EPISODE 138: 1.1310174465179443 seconds = 0.019 minutes\n",
      "\n",
      "Training on EPISODE 139 with epsilon 0.061545803450736285\n",
      "Episode:  139  terminated with Reward  164.0\n",
      "Time elapsed during EPISODE 139: 1.4269154071807861 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 140 with epsilon 0.060314887381721555\n",
      "Episode:  140  terminated with Reward  149.0\n",
      "Time elapsed during EPISODE 140: 1.5384254455566406 seconds = 0.026 minutes\n",
      "\n",
      "Training on EPISODE 141 with epsilon 0.05910858963408712\n",
      "Episode:  141  terminated with Reward  147.0\n",
      "Time elapsed during EPISODE 141: 1.4535577297210693 seconds = 0.024 minutes\n",
      "\n",
      "Training on EPISODE 142 with epsilon 0.05792641784140538\n",
      "Time elapsed during EPISODE 142: 4.967949628829956 seconds = 0.083 minutes\n",
      "\n",
      "Training on EPISODE 143 with epsilon 0.05676788948457727\n",
      "Episode:  143  terminated with Reward  210.0\n",
      "Time elapsed during EPISODE 143: 1.865900993347168 seconds = 0.031 minutes\n",
      "\n",
      "Training on EPISODE 144 with epsilon 0.055632531694885724\n",
      "Episode:  144  terminated with Reward  309.0\n",
      "Time elapsed during EPISODE 144: 2.9206740856170654 seconds = 0.049 minutes\n",
      "\n",
      "Training on EPISODE 145 with epsilon 0.054519881060988006\n",
      "Episode:  145  terminated with Reward  193.0\n",
      "Time elapsed during EPISODE 145: 1.8722212314605713 seconds = 0.031 minutes\n",
      "\n",
      "Training on EPISODE 146 with epsilon 0.05342948343976824\n",
      "Episode:  146  terminated with Reward  262.0\n",
      "Time elapsed during EPISODE 146: 2.3980214595794678 seconds = 0.04 minutes\n",
      "\n",
      "Training on EPISODE 147 with epsilon 0.052360893770972874\n",
      "Episode:  147  terminated with Reward  267.0\n",
      "Time elapsed during EPISODE 147: 2.3781259059906006 seconds = 0.04 minutes\n",
      "\n",
      "Training on EPISODE 148 with epsilon 0.05131367589555342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  148  terminated with Reward  201.0\n",
      "Time elapsed during EPISODE 148: 1.7284715175628662 seconds = 0.029 minutes\n",
      "\n",
      "Training on EPISODE 149 with epsilon 0.05028740237764235\n",
      "Episode:  149  terminated with Reward  220.0\n",
      "Time elapsed during EPISODE 149: 1.9449799060821533 seconds = 0.032 minutes\n",
      "\n",
      "Training on EPISODE 150 with epsilon 0.0492816543300895\n",
      "Episode:  150  terminated with Reward  294.0\n",
      "Time elapsed during EPISODE 150: 2.5956079959869385 seconds = 0.043 minutes\n"
     ]
    }
   ],
   "source": [
    "rewards, epsilon_values = list(), list() # Lists to keep logs of rewards and apsilon values, for plotting later\n",
    "time_step = 0 # Initalize timestep counter\n",
    "#we have 150 episodes and each one has 500 steps. \n",
    "for ep in range(num_episodes):\n",
    "    #we want to have the total rewards per episode after the 500 steps are done \n",
    "    tot_reward = 0\n",
    "    #we reset tne environment, start from the beginning every time we start a new episode. \n",
    "    state, _ = env.reset()\n",
    "    #+1 because it starts at 0. \n",
    "    print(f'\\nTraining on EPISODE {ep+1} with epsilon {dqn_agent.epsilon}')\n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        time_step += 1\n",
    "        #update_rate = 10, we get the weights from the main_network but we do not update the taget model in every step\n",
    "        #we do it every 10 steps so the model has learnt something with 10 steps rather than fit evey single step. \n",
    "        # Update Target Network every {dqn_agent.update_rate} timesteps\n",
    "        if time_step % dqn_agent.update_rate == 0:\n",
    "            dqn_agent.update_target_network()\n",
    "            \n",
    "        action = dqn_agent.pick_epsilon_greedy_action(state) # Select action with ε-greedy policy,could be random or maxQvalue.\n",
    "        next_state, reward, terminal, _, _ = env.step(action) # Perform action on environment\n",
    "        #get the experience with the maximum q-value. \n",
    "        dqn_agent.save_experience(state, action, reward, next_state, terminal) # Save experience in ReplayBuffer\n",
    "        #in the next step we have a new state and the rewards are adding in each step iteration for the same episode. \n",
    "        state = next_state\n",
    "        tot_reward += reward\n",
    "        #an episode will be terminated if:\n",
    "        #The Cart position on the x axis leaves the (-2.4, 2.4) range The Pole angle leaves the (-0.2095, 0.2095) \n",
    "        #range Rewards The agent receives a reward of +1 for each time step, \n",
    "        #with the intention of keeping the pole standing for as long as possible.\n",
    "        if terminal:\n",
    "            print('Episode: ', ep+1, ' terminated with Reward ', tot_reward)\n",
    "            break\n",
    "        #save_experience function appends into replay_buffer each experience, step. \n",
    "        # Train the Main Network when ReplayBuffer has enough experiences to fill a batch\n",
    "        if len(dqn_agent.replay_buffer) > batch_size:\n",
    "            dqn_agent.train(batch_size)\n",
    "    #you have the total rewards per episode.\n",
    "    #the epsilon is the same  in each step, Epsilon starts yo decay once a new episode starts. \n",
    "    rewards.append(tot_reward)\n",
    "    epsilon_values.append(dqn_agent.epsilon)\n",
    "    \n",
    "     # Everytime an episode is finished, update Epsilon value to a lower value\n",
    "    if dqn_agent.epsilon > dqn_agent.epsilon_min:\n",
    "        dqn_agent.epsilon *= dqn_agent.epsilon_decay\n",
    "    \n",
    "    # Print information about the Episode performed\n",
    "    elapsed = time.time() - start\n",
    "    print(f'Time elapsed during EPISODE {ep+1}: {elapsed} seconds = {round(elapsed/60, 3)} minutes')\n",
    "    \n",
    "    # If the agent got a reward >499 in each of the last 10 episodes, the training is terminated\n",
    "    #499*10=4990. \n",
    "    if sum(rewards[-10:]) > 4990:\n",
    "        print('Training stopped because agent has performed a perfect episode in the last 10 episodes')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a174c35",
   "metadata": {},
   "source": [
    "as you can see above all the 150 episodes have run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9f709",
   "metadata": {},
   "source": [
    "To highlight there was not an early stop of the iteration and iterates of the total of 150 episodes, \n",
    "because the agent has not performed a perfect episode in the last 10 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412eb1a",
   "metadata": {},
   "source": [
    "5. Test the trained by executing an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a19ff0",
   "metadata": {},
   "source": [
    "test if the agent after training the model has learnt to do the right actions in an episode without early termination. \n",
    "recap, every right step gets +1 reward if all the steps do not fail we get a total of reward = 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d41b07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_trained_agent_action(state, trained_model):\n",
    "    q_values = trained_model.predict(state, verbose=0)\n",
    "    return np.argmax(q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fd9db1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode terminated with reward 486.0 (Max Reward=500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anacondalastone\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "state, _ = env.reset()\n",
    "state_size = env.observation_space.shape[0]\n",
    "total_reward = 0\n",
    "max_timesteps = 500\n",
    "\n",
    "# Execute episode\n",
    "for t in range(max_timesteps):\n",
    "    \n",
    "    state = state.reshape((1, state_size))\n",
    "    action = select_trained_agent_action(state, dqn_agent.main_network)\n",
    "    next_state, reward, terminal, _, _ = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    \n",
    "env.close()\n",
    "    \n",
    "print(f'Test episode terminated with reward {total_reward} (Max Reward=500)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d32f6d",
   "metadata": {},
   "source": [
    "# Compare this value with one training episode from above such as: Training on EPISODE 50 with epsilon 0.37. Episode:  50  terminated with Reward  227.0. This proves that the agent has learnt to take the right actions in each step to reach the maximum reward, aprox 500, without early termination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d1a7aea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16.0,\n",
       " 18.0,\n",
       " 12.0,\n",
       " 19.0,\n",
       " 30.0,\n",
       " 25.0,\n",
       " 15.0,\n",
       " 18.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 24.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 26.0,\n",
       " 15.0,\n",
       " 17.0,\n",
       " 23.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 13.0,\n",
       " 20.0,\n",
       " 29.0,\n",
       " 27.0,\n",
       " 22.0,\n",
       " 53.0,\n",
       " 47.0,\n",
       " 11.0,\n",
       " 77.0,\n",
       " 41.0,\n",
       " 97.0,\n",
       " 10.0,\n",
       " 133.0,\n",
       " 102.0,\n",
       " 53.0,\n",
       " 83.0,\n",
       " 183.0,\n",
       " 97.0,\n",
       " 127.0,\n",
       " 275.0,\n",
       " 339.0,\n",
       " 264.0,\n",
       " 232.0,\n",
       " 267.0,\n",
       " 281.0,\n",
       " 227.0,\n",
       " 232.0,\n",
       " 245.0,\n",
       " 268.0,\n",
       " 210.0,\n",
       " 269.0,\n",
       " 205.0,\n",
       " 349.0,\n",
       " 408.0,\n",
       " 247.0,\n",
       " 196.0,\n",
       " 306.0,\n",
       " 280.0,\n",
       " 283.0,\n",
       " 251.0,\n",
       " 362.0,\n",
       " 209.0,\n",
       " 155.0,\n",
       " 229.0,\n",
       " 170.0,\n",
       " 151.0,\n",
       " 152.0,\n",
       " 223.0,\n",
       " 335.0,\n",
       " 238.0,\n",
       " 168.0,\n",
       " 364.0,\n",
       " 261.0,\n",
       " 262.0,\n",
       " 220.0,\n",
       " 301.0,\n",
       " 309.0,\n",
       " 273.0,\n",
       " 191.0,\n",
       " 210.0,\n",
       " 227.0,\n",
       " 248.0,\n",
       " 241.0,\n",
       " 500.0,\n",
       " 500.0,\n",
       " 498.0,\n",
       " 305.0,\n",
       " 202.0,\n",
       " 208.0,\n",
       " 215.0,\n",
       " 208.0,\n",
       " 258.0,\n",
       " 242.0,\n",
       " 197.0,\n",
       " 197.0,\n",
       " 200.0,\n",
       " 173.0,\n",
       " 172.0,\n",
       " 177.0,\n",
       " 181.0,\n",
       " 178.0,\n",
       " 182.0,\n",
       " 166.0,\n",
       " 167.0,\n",
       " 192.0,\n",
       " 219.0,\n",
       " 201.0,\n",
       " 241.0,\n",
       " 240.0,\n",
       " 208.0,\n",
       " 252.0,\n",
       " 183.0,\n",
       " 228.0,\n",
       " 219.0,\n",
       " 351.0,\n",
       " 199.0,\n",
       " 261.0,\n",
       " 296.0,\n",
       " 249.0,\n",
       " 252.0,\n",
       " 284.0,\n",
       " 332.0,\n",
       " 500.0,\n",
       " 500.0,\n",
       " 500.0,\n",
       " 500.0,\n",
       " 500.0,\n",
       " 500.0,\n",
       " 488.0,\n",
       " 125.0,\n",
       " 337.0,\n",
       " 148.0,\n",
       " 154.0,\n",
       " 128.0,\n",
       " 164.0,\n",
       " 149.0,\n",
       " 147.0,\n",
       " 500.0,\n",
       " 210.0,\n",
       " 309.0,\n",
       " 193.0,\n",
       " 262.0,\n",
       " 267.0,\n",
       " 201.0,\n",
       " 220.0,\n",
       " 294.0]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rewards should increase per episode because since epsilon start decaying the agent starts taking not random actions\n",
    "#and thus takes actions based on the max q-value. \n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9a020fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards)#total of episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b42d2c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.98,\n",
       " 0.9603999999999999,\n",
       " 0.9411919999999999,\n",
       " 0.9223681599999999,\n",
       " 0.9039207967999998,\n",
       " 0.8858423808639998,\n",
       " 0.8681255332467198,\n",
       " 0.8507630225817854,\n",
       " 0.8337477621301497,\n",
       " 0.8170728068875467,\n",
       " 0.8007313507497957,\n",
       " 0.7847167237347998,\n",
       " 0.7690223892601038,\n",
       " 0.7536419414749017,\n",
       " 0.7385691026454037,\n",
       " 0.7237977205924956,\n",
       " 0.7093217661806457,\n",
       " 0.6951353308570327,\n",
       " 0.6812326242398921,\n",
       " 0.6676079717550942,\n",
       " 0.6542558123199923,\n",
       " 0.6411706960735924,\n",
       " 0.6283472821521205,\n",
       " 0.6157803365090782,\n",
       " 0.6034647297788965,\n",
       " 0.5913954351833186,\n",
       " 0.5795675264796523,\n",
       " 0.5679761759500592,\n",
       " 0.5566166524310581,\n",
       " 0.5454843193824369,\n",
       " 0.5345746329947881,\n",
       " 0.5238831403348924,\n",
       " 0.5134054775281945,\n",
       " 0.5031373679776306,\n",
       " 0.493074620618078,\n",
       " 0.48321312820571644,\n",
       " 0.4735488656416021,\n",
       " 0.46407788832877006,\n",
       " 0.45479633056219465,\n",
       " 0.44570040395095073,\n",
       " 0.4367863958719317,\n",
       " 0.42805066795449304,\n",
       " 0.41948965459540316,\n",
       " 0.4110998615034951,\n",
       " 0.4028778642734252,\n",
       " 0.39482030698795667,\n",
       " 0.38692390084819756,\n",
       " 0.3791854228312336,\n",
       " 0.37160171437460887,\n",
       " 0.3641696800871167,\n",
       " 0.35688628648537435,\n",
       " 0.34974856075566685,\n",
       " 0.3427535895405535,\n",
       " 0.33589851774974244,\n",
       " 0.3291805473947476,\n",
       " 0.32259693644685267,\n",
       " 0.3161449977179156,\n",
       " 0.3098220977635573,\n",
       " 0.30362565580828615,\n",
       " 0.2975531426921204,\n",
       " 0.291602079838278,\n",
       " 0.2857700382415124,\n",
       " 0.2800546374766822,\n",
       " 0.27445354472714856,\n",
       " 0.2689644738326056,\n",
       " 0.26358518435595346,\n",
       " 0.25831348066883436,\n",
       " 0.2531472110554577,\n",
       " 0.24808426683434853,\n",
       " 0.24312258149766156,\n",
       " 0.2382601298677083,\n",
       " 0.23349492727035415,\n",
       " 0.22882502872494706,\n",
       " 0.22424852815044813,\n",
       " 0.21976355758743915,\n",
       " 0.21536828643569036,\n",
       " 0.21106092070697655,\n",
       " 0.20683970229283702,\n",
       " 0.20270290824698028,\n",
       " 0.19864885008204067,\n",
       " 0.19467587308039985,\n",
       " 0.19078235561879187,\n",
       " 0.18696670850641603,\n",
       " 0.18322737433628772,\n",
       " 0.17956282684956196,\n",
       " 0.1759715703125707,\n",
       " 0.1724521389063193,\n",
       " 0.16900309612819292,\n",
       " 0.16562303420562907,\n",
       " 0.16231057352151648,\n",
       " 0.15906436205108615,\n",
       " 0.15588307481006441,\n",
       " 0.15276541331386312,\n",
       " 0.14971010504758586,\n",
       " 0.14671590294663414,\n",
       " 0.14378158488770146,\n",
       " 0.14090595318994742,\n",
       " 0.13808783412614847,\n",
       " 0.13532607744362551,\n",
       " 0.132619555894753,\n",
       " 0.12996716477685794,\n",
       " 0.12736782148132078,\n",
       " 0.12482046505169436,\n",
       " 0.12232405575066048,\n",
       " 0.11987757463564727,\n",
       " 0.11748002314293432,\n",
       " 0.11513042268007563,\n",
       " 0.11282781422647412,\n",
       " 0.11057125794194463,\n",
       " 0.10835983278310574,\n",
       " 0.10619263612744362,\n",
       " 0.10406878340489474,\n",
       " 0.10198740773679685,\n",
       " 0.09994765958206091,\n",
       " 0.0979487063904197,\n",
       " 0.0959897322626113,\n",
       " 0.09406993761735907,\n",
       " 0.0921885388650119,\n",
       " 0.09034476808771166,\n",
       " 0.08853787272595742,\n",
       " 0.08676711527143827,\n",
       " 0.0850317729660095,\n",
       " 0.08333113750668932,\n",
       " 0.08166451475655553,\n",
       " 0.08003122446142442,\n",
       " 0.07843059997219594,\n",
       " 0.07686198797275202,\n",
       " 0.07532474821329699,\n",
       " 0.07381825324903105,\n",
       " 0.07234188818405042,\n",
       " 0.07089505042036941,\n",
       " 0.06947714941196202,\n",
       " 0.06808760642372277,\n",
       " 0.06672585429524831,\n",
       " 0.06539133720934334,\n",
       " 0.06408351046515648,\n",
       " 0.06280184025585335,\n",
       " 0.061545803450736285,\n",
       " 0.060314887381721555,\n",
       " 0.05910858963408712,\n",
       " 0.05792641784140538,\n",
       " 0.05676788948457727,\n",
       " 0.055632531694885724,\n",
       " 0.054519881060988006,\n",
       " 0.05342948343976824,\n",
       " 0.052360893770972874,\n",
       " 0.05131367589555342,\n",
       " 0.05028740237764235,\n",
       " 0.0492816543300895]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon_values#epsilon decreases in every episode due to the decay rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cf140d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(epsilon_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8223c",
   "metadata": {},
   "source": [
    "There are certain behaviors of the code worth mentioning:\n",
    "\n",
    "The update_target_network() method, which has not been mentioned in this article, copies the weights from the main neural network to the target neural network. This process is repeated every {update_rate} timesteps.\n",
    "Training of the main neural network is only performed when there are enough experiences in the Replay Buffer to fill a batch.\n",
    "The epsilon value is reduced in each episode as long as a minimum value has not been reached, by multiplying the previous value by a reducing factor less than 1.\n",
    "Training stops when a cumulative reward of more than 499 has been reached in each of the last 10 episodes, as the agent is considered to have learned to perform the task successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548490d",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a918ba",
   "metadata": {},
   "source": [
    "# Test the DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38ea3f",
   "metadata": {},
   "source": [
    "The performance of the trained DQN agent is evaluated by plotting the rewards obtained during training, and by running a test episode.\n",
    "\n",
    "The reward plot is used to see if the agent is able to make the rewards converge towards the maximum, or if on the contrary it is not able to make the rewards increase with each training episode, which would mean that the training has failed. As for the execution of the test episode, it is a realistic way to evaluate the agent, since it shows in a practical way whether the agent has really learned to perform the task for which he has been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672ab5b",
   "metadata": {},
   "source": [
    "The graph clearly shows how the rewards converge towards the maximum, despite the fact that in some episodes the agent fails in a few timesteps. This may occur because in those episodes the agent is initialized in a position that it does not know well, since it has not been initialized that way many times throughout the training. Likewise, the tendency of the rewards to maximum values is an indicator that the agent has trained correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d4c761a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot rewards: \n",
    "def plot_reward_values():\n",
    "    \n",
    "    plt.plot(range(len(rewards)), rewards)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba09535",
   "metadata": {},
   "source": [
    "# Check how the total rewards improve after around 45 episodes, then it oscilates as we have explained before but it has reached the maximum rewards of 500 in many episodes. That would not be possible without training the agent to make the right actions guided by the maximum Q-value in each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f63a6489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_reward_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8c7c33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot epsilon: \n",
    "def plot_epsilon_values():\n",
    "    plt.plot(range(len(epsilon_values)), epsilon_values)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon_Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b50dd3",
   "metadata": {},
   "source": [
    "# updates the epsilon value in each episode, making it lower and lower. This makes the exploration phase happen mainly at the beginning of the training, and the exploitation phase in the remaining episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3883fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEGCAYAAACHGfl5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAoDklEQVR4nO3dd3yV9d3/8dcniwySkJAQIGFKWAooRFBRq+LCrVhXHaXDG7e2/d3WWnu33neHHY5a696z7lWr1okLJCAblBBWACGMBAgEEvL5/XEONsZAEszJdZLzfj4e55Fzrus6yRtIeOe6vtf1vczdERGR2BYXdAAREQmeykBERFQGIiKiMhAREVQGIiICJAQdYG/k5OR43759g44hItKuTJ8+fZ275za2rl2WQd++fSkuLg46hohIu2Jmy3a3ToeJREREZSAiIioDERFBZSAiIqgMRESECJeBmT1gZmvNbO5u1puZ/dXMSsxstpmNjGQeERFpXKT3DB4Cjt/D+vFAYfhxMXBnhPOIiEgjIloG7j4Z2LCHTU4FHvGQKUAXM+sRqTwLVm/i968tQNN2i4h8XdBjBvnAinqvy8LLvsHMLjazYjMrLi8v36svNmtFBXdPLuW9z/fu/SIiHVXQZWCNLGv013Z3v8fdi9y9KDe30aupmzRhVAEFWSnc8tYX2jsQEakn6DIoA3rVe10ArIrUF0uMj+OKowYwu6ySdz9fG6kvIyLS7gRdBi8DF4bPKjoIqHT31ZH8gmeMDO0d3PrWIu0diIiERfrU0ieBT4BBZlZmZj80s0lmNim8yWtAKVAC3AtcGsk88PW9g3cWau9ARAQiPGupu5/bxHoHLotkhsacMbKAv71bwq1vLeKowd0wa2zoQkQkdgR9mCgQifFxXHFkIXNWau9ARARitAwATh+ZT+/sVI0diIgQw2WQGB/H5UcNYM7KSt5eoL0DEYltMVsGAKcfEN47eFvXHYhIbIvpMti1dzB35SbtHYhITIvpMgDtHYiIgMrgq+sO5q7cxBvz1gQdR0QkEDFfBhDaO9gnN40/v/k5tTvrgo4jItLmVAZAQnwc/++4QZSs3cLzM1YGHUdEpM2pDMKO27c7I3p14Za3vqC6ZmfQcURE2pTKIMzMuPb4QayurOaxKcuCjiMi0qZUBvUcsk8OhxXm8Ld3S9hUXRN0HBGRNqMyaODa4wdTsbWGeyeXBh1FRKTNqAwa2C8/k5OG9+C+D5ZQvnl70HFERNqEyqARPz12EDt21nH7O4uCjiIi0iZUBo3ol5PG2Qf24ompy1m+fmvQcUREIk5lsBtXjSskId64+d+fBx1FRCTiVAa7kZeRzMSx/Xhp1irmraoMOo6ISESpDPZg0uH7kJmSyG//uUCT2IlIh6Yy2IPM1ESuHlfIx4vXa4prEenQVAZN+N5BfdgnN43fvbaAHbWaxE5EOiaVQRMS4+O4/sQhlK6r0jQVItJhqQya4chB3TisMIfb3l5ExdYdQccREWl1KoNmMDN+eeJQNlfXcOtbuhBNRDoelUEzDeqezjmje/PYlGUsLt8SdBwRkValMmiBnxwzkJTEeH7/2oKgo4iItCqVQQvkdO7EZUcN4K0Fa/moZF3QcUREWo3KoIW+f0hfCrJS+N9X57OzTheiiUjHoDJooeTEeK4bP4SFX27mH9NWBB1HRKRVqAz2wgnDujOmXzZ/fGMhG6t0qqmItH8qg71gZtx46n5srq7lj29oVlMRaf9UBntpUPd0Jh7Sl6emLWfWioqg44iIfCsqg2/hqqMLye3ciRtemqvBZBFp1yJeBmZ2vJl9bmYlZvbzRtZnmtkrZjbLzOaZ2cRIZ2ot6cmJXH/iEGaXVWowWUTatYiWgZnFA3cA44GhwLlmNrTBZpcB8919BHAE8BczS4pkrtZ0yoieHNQ/NJi8QYPJItJORXrPYDRQ4u6l7r4DeAo4tcE2DqSbmQGdgQ1AbYRztZpdg8lbqmv50xsLg44jIrJXIl0G+UD94ydl4WX1/Q0YAqwC5gBXufs3bhxgZhebWbGZFZeXl0cq714ZmJfOxLF9eWraCj5bvjHoOCIiLRbpMrBGljUcaT0OmAn0BPYH/mZmGd94k/s97l7k7kW5ubmtnfNbu+rogXRL78SvXpqnwWQRaXciXQZlQK96rwsI7QHUNxF43kNKgCXA4AjnanWdOyVw/YlDmbOykic+XR50HBGRFol0GUwDCs2sX3hQ+Bzg5QbbLAfGAZhZHjAIKI1wrog4eXgPxg7oyh//tZA1m6qDjiMi0mwRLQN3rwUuB94AFgBPu/s8M5tkZpPCm/0vcIiZzQHeBq5193Y5JaiZ8dvThrFjZx2/emlu0HFERJotIdJfwN1fA15rsOyues9XAcdGOkdb6ZuTxjXHDOQP/1rI63NXc/x+PYKOJCLSJF2BHAE/OrQfQ3tk8KuX5lG5rSboOCIiTVIZREBCfBw3TRjOui3buel1XXsgItFPZRAhwwoy+eGh/Xhi6nKmlq4POo6IyB6pDCLommMG0is7hetemEN1zc6g44iI7JbKIIJSkxL43enDKC2v4o53S4KOIyKyWyqDCDusMJczRuZz53uLWfjlpqDjiIg0SmXQBm44cSiZKYlc++xsand+Y9olEZHAqQzaQFZaEjeeuh+zyiq56/3FQccREfkGlUEbOXF4D04e0ZPb3l7E/FU6XCQi0UVl0IZuPGVfuqQm8ZOnZ7KjVoeLRCR6qAzaUFZaEn84YxgLv9zMX99eFHQcEZGvqAza2LgheXx3VAF/f69EN8IRkaihMgjADScPpXtGMj99ZpYuRhORqKAyCEBGciJ/PHMEpeVV/PmNz4OOIyKiMgjKoYU5XHBQH+7/aInmLhKRwKkMAvTz8YPpnZ3KT5+ZxaZqTXUtIsFRGQQorVMCN5+1P6srq7nhxbm4e9CRRCRG7VUZmFmWmQ1v7TCxaFSfLK4eV8hLM1fxwmcrg44jIjGq2WVgZu+ZWYaZZQOzgAfN7ObIRYsdlx45gNH9srnhxbksXVcVdBwRiUEt2TPIdPdNwBnAg+4+Cjg6MrFiS3yccevZ+xMfZ1z11Ge6OllE2lxLyiDBzHoAZwGvRihPzOrZJYWbJgxnVlklt7z1RdBxRCTGtKQMbgTeABa7+zQz6w9oToVWNH5YD84d3Yu73l/MxyXrgo4jIjGk2WXg7s+4+3B3vyT8utTdJ0QuWmy64aSh9M9J45qnZ7KhakfQcUQkRrRkAHmgmb1tZnPDr4eb2S8jFy02pSYl8NdzD2BjVQ3//exsnW4qIm2iJYeJ7gWuA2oA3H02cE4kQsW6fXtm8vPxg3lrwRru+2BJ0HFEJAa0pAxS3f3TBstqWzOM/MfEsX05ft/u/OH1hUxbuiHoOCLSwbWkDNaZ2T6AA5jZmcDqiKQSzIw/fnc4vbJSuOzxGZRv3h50JBHpwFpSBpcBdwODzWwlcDVwSSRCSUhGciJ//94oKrfVcNVTn7GzTuMHIhIZLTmbqNTdjwZygcHufqi7L41YMgFgaM8M/u+0/fh48Xpu1fUHIhIhCc3d0Mx+1eA1AO5+Yytnkga+W9SL4qUbuf2dEkb2yeLIQd2CjiQiHUxLDhNV1XvsBMYDfSOQSRrxm1P3ZUiPDK75x0zKNm4NOo6IdDAtOUz0l3qP3wJHAPkRSyZfk5wYz53fG8nOnc5lj89ge61ulykirefb3M8gFejf1EZmdryZfW5mJWb2891sc4SZzTSzeWb2/rfI1KH1zUnjz2eNYFZZJb98Qfc/EJHW05IxgzmETysF4gkNJO9xvMDM4oE7gGOAMmCamb3s7vPrbdMF+DtwvLsvNzMdEN+D4/btzpXjCvnr24sY2jODiWP7BR1JRDqAZpcBcFK957XAGndv6qKz0UCJu5cCmNlTwKnA/HrbnAc87+7LAdx9bQsyxaSrxxWycPUm/u+fCxiYl87YATlBRxKRdq7Jw0Rmlh2+oc3meo9twK4b3exJPrCi3usyvjnOMBDICt88Z7qZXbibHBebWbGZFZeXlzcVu0OLizNuPnt/9slN49LHZ7BsvW6IIyLfTnPGDKYDxeGPDR/FTbzXGlnW8EB3AjAKOBE4DrjBzAZ+403u97h7kbsX5ebmNiN2x9a5UwL3XlgEwI8fKWbLds0MIiJ7r8kycPd+7t4//LHho6kB5DKgV73XBcCqRrZ53d2r3H0dMBkY0ZI/RKzq0zWNO84byeLyKn7yj5nU6QplEdlLLTqbyMyyzGy0mR2+69HEW6YBhWbWz8ySCM1y+nKDbV4CDjOzBDNLBcYAC1qSK5YdWpjD9ScM4c35a7jtbd1rSET2TkvOJvoRcBWh3+5nAgcBnwBH7e497l5rZpcTukNaPPCAu88zs0nh9Xe5+wIzex2YDdQB97n73L3888SkiWP7Mn/1Jm57exEDunXm5BE9g44kIu2MNfdc9fCppQcCU9x9fzMbDPzG3c+OZMDGFBUVeXFxU8MVsWV77U7Ov28qs8oqeeJHYyjq29TYvojEGjOb7u5Fja1ryWGianevDn/CTu6+EBjUGgHl2+uUEM89FxSR3yWFHz9SzNJ1OsNIRJqvJWVQFr5A7EXg32b2Et8cDJYAZaUl8eD3DwRg4kPT2Kh7KItIMzXnOoOfmVkvdz/d3Svc/dfADcD9wGkRzict1DcnjfsuKmJlxTYufrSY6hrNYSQiTWvOnkE+8LGZTTazS8wsx93fd/eX3V2/ekahUX2yufmsEUxbupH/9+xsnXIqIk1qznUG1wC9Ce0NDAdmm9m/zOxCM0uPdEDZOycN78m1xw/mlVmr+Mu/Pw86johEuWaNGXjI++5+CaGLyG4FrgHWRDCbfEuTvtOfc0f34o53F/P41GVBxxGRKNaSieows2GELhw7G1gP/CISoaR1mBk3nrofazZt55cvziUrNYkThvUIOpaIRKHmDCAXmtkNZjYfeALYChzr7mPc/dZIB5RvJzE+jjvOG8mo3llc/dRMPipZF3QkEYlCzTlM9AaQDJzt7sPc/be7pqSW9iElKZ77LzqQ/rlpXPxIMbNWVAQdSUSiTHMGkPu7+/XuPmdP25nZJ60XS1pbZmoiD/9gNFlpSXz/wU8pWbsl6EgiEkW+zW0vG0puxc8lEZCXkcxjPxxDfJxx4f1TWVWxLehIIhIlWrMMdDJ7O9A3J42HJo5mc3UtF9w/lQ26SllEaN0ykHZiv/xM7r2oiBUbt3HB/VOp3FoTdCQRCVhrlkFjdzWTKHVQ/67cff4ovlizmQsfmMqmahWCSCxrzTK4oBU/l7SBIwd3447zRjJv1SYmPjhNt84UiWHNLgMzO8PMFplZpZltMrPNZrZp13rdkKZ9Onbf7tx+7gHMXFHBDx6axtYdKgSRWNSSPYM/Aqe4e6a7Z7h7urtnRCqYtJ3xw3pw81kjKF66gR89rJlORWJRS8pgjbvr3sQd1Kn75/OnM0fwSel6Ln50ugpBJMa0ZG6iYjP7B6Gb22zftdDdn2/tUBKMCaMKqK2r49rn5jDpsencdf4okhPjg44lIm2gJWWQQXheonrLHFAZdCBnH9ibOodfvDCHHzw0jfsuKiI1qUXzGYpIO9Tsn3J3nxjJIBI9zh3dm04JcfzsmVlceP+nPDjxQNKTE4OOJSIR1JKziQrM7AUzW2tma8zsOTMriGQ4Cc4ZIwu4/dyRzFxRwfn3TaViq65UFunIWjKA/CDwMtCT0K0wXwkvkw7qxOE9uPP8USxYvZlz753K+i3bm36TiLRLLSmDXHd/0N1rw4+HgNwI5ZIocczQPO69qIjS8i2cc88U1m6qDjqSiERAS8pgnZmdb2bx4cf5hO52Jh3cdwbm8tDE0ays2MaZd33CsvVVQUcSkVbWkjL4AXAW8CWwGjgzvExiwMH7dOXxH41hc3UNE+78hLkrK4OOJCKtqNll4O7L3f0Ud891927ufpq76y7rMeSA3lk8M+kQkuKNc+6ZwseLdQtNkY7C3Pd8GwIzu5093KvA3a9s7VBNKSoq8uLi4rb+shK2unIbFz3wKUvXbeXWc/bnhGE9go4kIs1gZtPdvaixdc3ZMygGpu/hITGmR2YKz/zXIQwvyOSyJ2bw6BTtIIq0d01edObuD7dFEGlfMlMTefSHY7jiyRnc8OJcyjdVc80xAzHTbS1E2qMmy8DMbnX3q83sFRo5XOTup0QkmUS9lKR47jp/FL94YQ5/faeE5Ru2ctOZw+mUoPmMRNqb5kxH8Wj4458jGUTap4T4OG6aMJw+XdP40xufs7JiG3dfUER2WlLQ0USkBZocM3D36eGP7+96ALOBjeHnEuPMjMuOHMDfzjuAWWWVnP73j1hcviXoWCLSAi2Zm+g9M8sws2xgFvCgmd3cjPcdb2afm1mJmf18D9sdaGY7zezM5maS6HLS8J48dfFBbKmu5fQ7PtKppyLtSEsuOst0903AGcCD7j4KOHpPbzCzeOAOYDwwFDjXzIbuZrubgDdakEei0MjeWbx42VjyMpK58P5Pebp4RdCRRKQZWlIGCWbWg9BVyK828z2jgRJ3L3X3HcBTwKmNbHcF8BywtgV5JEr1yk7l2UsO4eB9uvLfz87mxlfmU7uzLuhYIrIHLSmDGwn95r7Y3aeZWX9gURPvyQfq/2pYFl72FTPLB04H7trTJzKzi82s2MyKy8vLWxBbgpCZksiD3z+QH4ztxwMfLeHCBz5lQ5WmwRaJVi2ZjuIZdx/u7peEX5e6+4Qm3tbYSecNT0+9FbjW3fd40113v8fdi9y9KDdXk6W2Bwnxcfzq5KH85bsjKF62kZNv/5B5qzSnkUg0askAcn8ze8XMysM3uHnJzPo18bYyoFe91wXAqgbbFAFPmdlSQpPf/d3MTmtuLol+E0YV8Oykg6lzZ8KdH/PSzJVBRxKRBlpymOgJ4GmgB6Eb3DxDaAxgT6YBhWbWz8ySgHMI3SDnK+7ez937untf4FngUnd/sQW5pB0YXtCFly8/lOH5XbjqqZn87rUFGkcQiSItKQNz90fr3dzmMfYwgR2Au9cClxMaa1gAPO3u88xskplN2vvY0h7lpnfisR+N4YKD+nDP5FLOu28qa3SzHJGo0OSspV9taPYHoILQ3oADZwOdCJ06irtviEzEb9Kspe3fC5+V8Yvn55KaFM9t5xzAoYU5QUcS6fD2NGtpS8pgyR5Wu7v335twe0Nl0DEsWrOZSx+fQUn5Fq4aV8gVRxUSH6eJ7kQiZU9l0Jy5iYDQsf3WiyQChXnpvHT5WH754lxufWsRxUs3cus5+5PTuVPQ0URiTpNjBmb23/Wef7fBut9FIpTEjtSkBP7y3RH8ccJwpi3dwAm3fcDHJZrGQqStNWcA+Zx6z69rsO74VswiMcrMOOvAXrx42VjSkxP43v1T+f1rC9hRq7ONRNpKc8rAdvO8sdcie21IjwxeveIwzhvdm7snl3L63z+iZK1mPxVpC80pA9/N88Zei3wrKUnx/Pb0YdxzwShWVWzjpNs/4PGpy2juiQ4isneaM4A8wsw2EdoLSAk/J/w6OWLJJKYdu2939u/VhZ8+M4vrX5jLuwvLuWnCMLpqcFkkIppzc5t4d89w93R3Twg/3/U6sS1CSmzqlpHMwxNH88sThzD5i3KOvWUyr81ZHXQskQ6pJVcgi7S5uDjjR4f155UrDqVnlxQufXwGlz0xQzOgirQylYG0C4O6p/P8pYfws2MH8ua8Lzn2lvd5fa72EkRai8pA2o3E+DguP6qQV644lO6ZyUx6bAZXPvkZG7WXIPKtqQyk3RncPYMXLh3LT44ZyGtzVjPu5vd54bMynXEk8i2oDKRdSoyP48pxhbx65aH06ZrKNf+YxYUPfMqy9VVBRxNpl1QG0q4N7p7Bs5MO4X9P3ZfPlldw7C2TufO9xdToXgkiLaIykHYvPs644OC+vPWT73DkoG7c9PpCTr79Q6Yv2xh0NJF2Q2UgHUb3zGTuumAUd18wioqtNUy482N+9swsyjdvDzqaSNRTGUiHc9y+3Xnrp9/hv77Tn5dmruSov7zHgx8t0W02RfZAZSAdUudOCVw3fgivX304+/fqwm9emc+Jf/2QTxavDzqaSFRSGUiHtk9uZx75wWjuvmAUW7bXcu69U7jsiRms2LA16GgiUaXZdzoTaa/MjOP27c7hhbnc9f5i7p68mH/PW8PEsX259MgBZKZoii0R7RlIzEhJiueaYwby7s+O4OQRPbnng1KO+NO7PPzxUp2KKjFPZSAxp0dmCn85awSvXH4og7qn8z8vz+O4Wyfz5rwvdRWzxCyVgcSs/fIzefLHB3HfhUUAXPzodCbc+bEGmSUmqQwkppkZRw/N442rD+f3ZwxjVUU15947hQvun8rssoqg44m0GWuPu8VFRUVeXFwcdAzpgKprdvLYlGXc8W4JG7fWMH6/7vz02IEM6JYedDSRb83Mprt7UaPrVAYi37S5uob7P1zCfR8soWpHLScN78kVRw1gYJ5KQdovlYHIXtpQtYN7PyjlkY+XUrVjJ+P3687lRw1g356ZQUcTaTGVgci3tLFqBw9+tIQHP1rK5u21HD2kG1ccVciIXl2CjibSbCoDkVZSua2Ghz9eyv0fLqFyWw3fGZjLleMGMKpPdtDRRJqkMhBpZZura3h0yjLu+2AJG6p2cGDfLH58WH+OHpJHXJwFHU+kUSoDkQjZuqOWJz9dwQMfLmFlxTb65aTxw0P7ceaoApIT44OOJ/I1KgORCKvdWce/5n7JvR+UMruskuy0JC44qA8XHNyHnM6dgo4nAgRcBmZ2PHAbEA/c5+5/aLD+e8C14ZdbgEvcfdaePqfKQKKVu/Ppkg3c+0Epby1YS1JCHBNGFjBxbF+dliqB21MZRHTWUjOLB+4AjgHKgGlm9rK7z6+32RLgO+6+0czGA/cAYyKZSyRSzIwx/bsypn9XStZu4f4PS3luRhlPfrqcMf2yufDgvhy7bx6J8br4X6JLRPcMzOxg4Nfuflz49XUA7v773WyfBcx19/w9fV7tGUh7sqFqB/+YtoLHpixjZcU2uqV34tzRvTlvTG/yMpKDjicxJLA9AyAfWFHvdRl7/q3/h8C/IppIpI1lpyVxyRH7cPHh/Xnv87U88skybnt7EX97t4Tj9s3jgoP6clD/bMx0FpIEJ9Jl0Nh3d6O7ImZ2JKEyOHQ36y8GLgbo3bt3a+UTaTPxcca4IXmMG5LH0nVVPD51GU8Xl/HanC/pn5vGWUW9OGNkPt3StbcgbS8qDhOZ2XDgBWC8u3/R1OfVYSLpKLbt2Mkrs1fx9LQVFC/bSHycceSgbpxVVMCRg7tpbEFaVWBnE5lZAvAFMA5YCUwDznP3efW26Q28A1zo7h835/OqDKQjWly+haeLV/Dc9JWs27KdnM6dmDAqn++O6sWAbp2DjicdQNCnlp4A3Ero1NIH3P23ZjYJwN3vMrP7gAnAsvBbancXdheVgXRkNTvreP/zcv5RvIJ3Fq5lZ51zQO8unLZ/PicO76HrFmSv6aIzkXZq7eZqXpixkhc+W8nCLzcTH2ccVpjDqfv35Nih3UnrFOlhP+lIVAYiHcDnX27mxZkreXnmKlZWbCMlMZ5jhuZx2gE9OawwV+ML0iSVgUgHUlfnFC/byIszV/LanNVUbK2hS2oixwzJ44RhPRg7IIekBBWDfJPKQKSD2lFbx+QvyvnnnNW8NX8Nm7fXkp6cwDFD8jh+v+4cPjBXE+bJV4K86ExEIigpIY6jh+Zx9NA8ttfu5KOSdbw250v+PX8Nz3+2krSkeI4aksf4cDF01hiD7Ia+M0Q6iE4J8Rw1OI+jBudRs7OOTxav519zV/PGvDW8MmsVSfFxjOmfzTFDQxe+5XdJCTqyRBEdJhLp4Gp31lG8bCNvL1jDWwvWsmRdFQCDu6dz9JA8xg3pxoiCLropTwzQmIGIfGVx+ZaviqF46QbqHHI6d+LwwhwOH5jLoYU5upahg1IZiEijNlbt4L0v1vLOwnI+XFTOxq01AOyXn8FhhbkcXpjLqD5ZOjupg1AZiEiTdtY581ZVMvmLciYvWseMZRuprXNSk+I5uH9XDgvvOfTLSdMMq+2UykBEWmxzdQ1TSjcw+YtyPlhUztL1WwHokZnMmH7ZHNS/Kwf170qfrqkqh3ZCp5aKSIulJydyzNA8jhmaB8Dy9VuZvKicKaXr+bBkPS/OXAVA94xkxvT/Tzn0VTm0S9ozEJEWc3dK11UxpXQ9U0o3MKV0PeWbtwOQl9GJMf26cmC/bEb1zmJQ93TidaZSVNBhIhGJKHdnybqqr4phSul61obLIS0pnv17d2FU7yxG9snigN5ZZKYkBpw4NqkMRKRNuTtlG7cxY/lGpi8LPRZ+uZmddaH/bwq7dWZUnyxG9s7igN5d6J/bWXsPbUBlICKBq9pey6yyCmYs28iM5RVMX7aRym2hU1nTkuLZNz+T4fmZDCvIZHhBF/pkp+pCuFamAWQRCVxapwQO2SeHQ/bJAUKzr5auq2LWigrmrKxkVlkFj05ZxvbaOgDSkxMYtqsc8rswLD+TXtkpGpyOEJWBiAQiLs4Y0K0zA7p1ZsKoAiB0l7dFa7YwZ2UFs8sqmbOykgc+XELNztARjPROCQzukc6QHhkM6ZHB0B4ZDOqerplZW4HKQESiRmJ8HEN7ZjC0ZwZnHxhatr12J198uYU5KytZsHoTC1Zv4rnpZVTt2AlAnEG/nLSvFcTA7un0zEzWXkQLqAxEJKp1SohnWEHocNEudXXOio1bWbB6E/NXb2bB6k3MXFHBq7NXf7VNWlJ8eM8jncK8zgzM60xht3Tyu6RoLKIRGkAWkQ5jU3UNC1dvZtHazSxas4WStVtYtHYzazZt/2qb5MQ4BnQLFUPoY2f653amd3Zqh5+DSQPIIhITMpITGd0vm9H9sr+2vHJbTagY1mxm0dotLFq7haml63nhs5VfbRNnUJCVSt+cNPrnpNG3667nncnPSunwp76qDESkw8tMSWRUnyxG9cn62vLN1aGSWLKuiqXrqliyfitL1m1hxrKNbNle+9V2ifFGr+zUcEmk0ScnjV5ZKfTKTiW/S0qHGMBWGYhIzEpPTuSA3qGroutzd9Zt2VGvJKpYUl7F0vVVfFiyjuqauq9tn5fRiV5ZqfTKTqVXVgoF2anh1yn0yGwfexUqAxGRBsyM3PRO5KZ3+sYhp7o6p3zLdlZs2MryDVtZsWEbKzZuZcWGrXy6ZAMvzdxGXb2h2IQ4o2eXFHpkJn/1sUeXFHpmJtMjM4WeXZLJTEkM/MwnlYGISAvExRl5GcnkZSRT1Df7G+t31NaxunLb10qibOM2Vldu49MlG1izqZrauq+fuJOaFP/1sgiXRI/MFLpnJpOXnkxGSkJEC0NlICLSipIS4ujTNY0+XdMaXb+zzlm3ZTurKraxurL6q4+rK7exqqKaL9aUs3bzdhqe6JmcGEdeRjJXHFXImeGL9FqTykBEpA3F19uzOGA329TsrGPNpmpWV1bzZWU1azbtemyna+ekiORSGYiIRJnE+DgKslIpyEpts6/Zsa+wEBGRZlEZiIiIykBERFQGIiKCykBERFAZiIgIKgMREUFlICIitNOb25hZObBsL9+eA6xrxTiRoIytQxlbR7RnjPZ8ED0Z+7h7bmMr2mUZfBtmVry7O/1EC2VsHcrYOqI9Y7Tng/aRUYeJREREZSAiIrFZBvcEHaAZlLF1KGPriPaM0Z4P2kHGmBszEBGRb4rFPQMREWlAZSAiIrFVBmZ2vJl9bmYlZvbzoPMAmFkvM3vXzBaY2Twzuyq8PNvM/m1mi8IfswLOGW9mn5nZq1Gar4uZPWtmC8N/lwdHYcZrwv/Gc83sSTNLDjqjmT1gZmvNbG69ZbvNZGbXhX9+Pjez4wLM+Kfwv/VsM3vBzLpEW8Z6635mZm5mOUFmbErMlIGZxQN3AOOBocC5ZjY02FQA1AI/dfchwEHAZeFcPwfedvdC4O3w6yBdBSyo9zra8t0GvO7ug4ERhLJGTUYzyweuBIrcfT8gHjgnCjI+BBzfYFmjmcLfl+cA+4bf8/fwz1UQGf8N7Ofuw4EvgOuiMCNm1gs4Blheb1lQGfcoZsoAGA2UuHupu+8AngJODTgT7r7a3WeEn28m9J9YPqFsD4c3exg4LZCAgJkVACcC99VbHE35MoDDgfsB3H2Hu1cQRRnDEoAUM0sAUoFVBJzR3ScDGxos3l2mU4Gn3H27uy8BSgj9XLV5Rnd/091rwy+nALvuEB81GcNuAf4bqH+mTiAZmxJLZZAPrKj3uiy8LGqYWV/gAGAqkOfuqyFUGEC3AKPdSugbuq7esmjK1x8oBx4MH8q6z8zSoimju68E/kzoN8TVQKW7vxlNGevZXaZo/Rn6AfCv8POoyWhmpwAr3X1Wg1VRk7G+WCoDa2RZ1JxXa2adgeeAq919U9B5djGzk4C17j496Cx7kACMBO509wOAKoI/bPU14ePupwL9gJ5AmpmdH2yqFou6nyEzu57QodbHdy1qZLM2z2hmqcD1wK8aW93IssD/L4qlMigDetV7XUBoNz1wZpZIqAged/fnw4vXmFmP8PoewNqA4o0FTjGzpYQOrR1lZo9FUT4I/duWufvU8OtnCZVDNGU8Glji7uXuXgM8DxwSZRl32V2mqPoZMrOLgJOA7/l/LpiKloz7ECr+WeGfnQJghpl1J3oyfk0slcE0oNDM+plZEqEBnJcDzoSZGaFj3Qvc/eZ6q14GLgo/vwh4qa2zAbj7de5e4O59Cf2dvePu50dLPgB3/xJYYWaDwovGAfOJooyEDg8dZGap4X/zcYTGh6Ip4y67y/QycI6ZdTKzfkAh8GkA+TCz44FrgVPcfWu9VVGR0d3nuHs3d+8b/tkpA0aGv1ejIuM3uHvMPIATCJ15sBi4Pug84UyHEtpFnA3MDD9OALoSOpNjUfhjdhRkPQJ4Nfw8qvIB+wPF4b/HF4GsKMz4G2AhMBd4FOgUdEbgSUJjGDWE/sP64Z4yETr0sRj4HBgfYMYSQsfdd/3M3BVtGRusXwrkBJmxqYemoxARkZg6TCQiIruhMhAREZWBiIioDEREBJWBiIigMpAYZmY7zWxmvccer1o2s0lmdmErfN2l9WewFIkGOrVUYpaZbXH3zgF83aWEZi9d19ZfW2R3tGcg0kD4N/ebzOzT8GNAePmvzexn4edXmtn88Hz6T4WXZZvZi+FlU8xseHh5VzN7MzyJ3t3Um5vGzM4Pf42ZZna3he4bEW9mD1novgdzzOyaAP4aJMaoDCSWpTQ4THR2vXWb3H008DdCs7Y29HPgAA/Npz8pvOw3wGfhZb8AHgkv/x/gQw9Novcy0BvAzIYAZwNj3X1/YCfwPUJXU+e7+37uPgx4sLX+wCK7kxB0AJEAbQv/J9yYJ+t9vKWR9bOBx83sRULTX0BoapEJAO7+TniPIJPQvRbOCC//p5ltDG8/DhgFTAtNV0QKoUnhXgH6m9ntwD+BN/fyzyfSbNozEGmc7+b5LicSunPeKGB6+IY1e5qauLHPYcDD7r5/+DHI3X/t7hsJ3a3tPeAyvn5TIZGIUBmINO7seh8/qb/CzOKAXu7+LqGb/nQBOgOTCR3mwcyOANZ56N4U9ZePJzSJHoQmgTvTzLqF12WbWZ/wmUZx7v4ccAOh6bhFIkqHiSSWpZjZzHqvX3f3XaeXdjKzqYR+YTq3wfvigcfCh4AMuMXdK8zs14TutjYb2Mp/poH+DfCkmc0A3id8P1x3n29mvwTeDBdMDaE9gW3hz7Prl7XrWu1PLLIbOrVUpAGd+imxSIeJREREewYiIqI9AxERQWUgIiKoDEREBJWBiIigMhAREeD/A14ukb1e63H3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_epsilon_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243453ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af13a992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170a3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ef764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d095af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
